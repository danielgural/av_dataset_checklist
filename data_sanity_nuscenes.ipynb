{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sanity Checklist for Autonomous Vehicle Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autonomous vehicle datasets (e.g., nuScenes, Waymo, Wayve) often require conversion into a common format. After conversion, it’s crucial to verify the integrity and consistency of the data. Below is a comprehensive sanity-check checklist for multimodal autonomous driving data (images, LiDAR, etc.), with explanations and example code (using Python and the FiftyOne tool along with other libraries) for each check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Your FiftyOne Dataset In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset = fo.load_dataset(\"nuscenes-rerun-fo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ensure Metadata Completeness and Consistency\n",
    "\n",
    "What to check:\n",
    "\n",
    " Verify that each frame (sample) in the dataset has all essential metadata fields. This typically includes timestamps, sensor identifiers, sensor calibration parameters (intrinsics and extrinsics), and ego-vehicle pose (position and orientation). Such metadata makes the dataset “complete” and is needed for downstream tasks. \n",
    " \n",
    "For example, nuScenes stores localization (ego pose), timestamps, and calibration data for each frame as part of its metadata, and FiftyOne likewise allows attaching any metadata you need (time of day, device ID, location, weather, etc.) to each sample. \n",
    "\n",
    "Without these, it’s difficult to fuse sensor data or interpret coordinates properly. Why it’s important: \n",
    "\n",
    " Incomplete metadata can lead to misinterpretation of the data (e.g., misaligning sensors or time). Ensuring consistency (e.g., units and coordinate frames) is equally important. Every sample should at least have a timestamp and ego-vehicle pose so that frames can be ordered and spatially related. Calibration info (camera intrinsics, sensor extrinsics relative to the car) must be present to project between sensor frames correctly. If any of these are missing or incorrect, the subsequent sanity checks and any sensor fusion algorithm might fail. \n",
    "\n",
    "How to check: \n",
    "\n",
    " Use FiftyOne to iterate through the dataset and assert the presence of required fields. For instance, suppose the dataset’s Sample schema includes fields like timestamp, ego_pose (with position/orientation), and per-sensor calibration entries (intrinsics/extrinsics). We can programmatically verify these. If something is missing, we should flag it before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the names of your corresponding fields as the values in the dictionaries below.\n",
    "\n",
    "metadata_dict = {\n",
    "    \"timestamp\" : \"timestamp\",         # Mandatory\n",
    "    \"intrinsics\" : \"intrinsics\",       # Mandatory\n",
    "    \"T_rig_world\" : \"T_rig_world\",     # Mandatory\n",
    "    \"T_sensor_rig\" : \"T_sensor_rig\",   # Mandatory\n",
    "}\n",
    "\n",
    "optional_metadata_dict = {\n",
    "    \"location\" : \"location\",                          # Optional\n",
    "    \"velocity\" : \"velocity\",                          # Optional\n",
    "    \"angular_velocity\" : \"angular_velocity\",          # Optional\n",
    "    \"s_timestamp\" : \"s_timestamp\",                    # Optional\n",
    "    \"e_timestamp\" : \"e_timestamp\",                    # Optional\n",
    "    \"rig_timestamp\" : \"rig_timestamp\",                # Optional\n",
    "    \"ground_truth_cuboids\" : \"ground_truth_cuboids\",  # Optional\n",
    "    \"ego_mask\" : \"ego_mask\",                          # Optional\n",
    "    \"ground_truth_masks\" : \"ground_truth_masks\",      # Optional\n",
    "    \"ground_truth_lidar_segmentation\" : \"ground_truth_lidar_segmentation\",  # Optional\n",
    "    \"ego_rotation\" : \"ego_rotation\",                  # Optional\n",
    "    \"cs_rotation\" : \"cs_rotation\",                    # Optional\n",
    "    \"ego_translation\" : \"ego_translation\",            # Optional\n",
    "    \"cs_translation\" : \"cs_translation\",              # Optional\n",
    "}\n",
    "\n",
    "expected_sensors = [\"CAM_FRONT\", \"CAM_BACK\", \"CAM_FRONT_LEFT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_LEFT\", \"CAM_BACK_RIGHT\", \"3D\"] # Mandatory\n",
    "img_sensors = [\"CAM_FRONT\", \"CAM_BACK\", \"CAM_FRONT_LEFT\", \"CAM_FRONT_RIGHT\", \"CAM_BACK_LEFT\", \"CAM_BACK_RIGHT\"]  # Mandatory\n",
    "pcd_sensors = [\"LIDAR_TOP\", \"RADAR_FRONT\", \"RADAR_BACK_LEFT\", \"RADAR_FRONT_RIGHT\", \"RADAR_FRONT_LEFT\", \"RADAR_BACK_RIGHT\"]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Check for Missing Metadata Fields\n",
    "\n",
    "In this step, we programmatically verify the presence of required metadata fields in the dataset schema. Missing metadata fields can lead to issues in downstream tasks, such as sensor fusion or coordinate interpretation.\n",
    "\n",
    "The `check_metadata_fields` function iterates through the dataset's schema and checks for the presence of mandatory fields defined in `metadata_dict`. If any required fields are missing, they are flagged for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metadata_fields(dataset, metadata_dict):\n",
    "    # Get the field schema of the each slice\n",
    "\n",
    "    for sensor in expected_sensors:\n",
    "        view = dataset.select_group_slices(sensor)\n",
    "        field_schema = view.get_field_schema()\n",
    "        \n",
    "        # Check if each required metadata field exists in the schema\n",
    "        missing_fields = []\n",
    "        for key, field in metadata_dict.items():\n",
    "            if field not in field_schema:\n",
    "                missing_fields.append(key)\n",
    "        \n",
    "        # Return the result\n",
    "        if missing_fields:\n",
    "            print(f\"Missing metadata fields: {missing_fields} in {sensor} slice.\")\n",
    "        else:\n",
    "            print(\"All required metadata fields are present in the dataset schema.\")\n",
    "\n",
    "check_metadata_fields(dataset, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Identify Present Optional Metadata Fields\n",
    "\n",
    "Optional metadata fields can provide additional context or insights but are not strictly necessary for all tasks. The `get_present_optional_fields` function checks which optional fields, as defined in `optional_metadata_dict`, are present in the dataset schema. This helps in understanding the dataset's completeness and potential for extended analysis.\n",
    "\n",
    "The variable `present_optional_fields` stores the list of optional fields that are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_present_optional_fields(dataset, optional_metadata_dict):\n",
    "    # Get the field schema of the dataset\n",
    "\n",
    "    present_fields_dict = {}\n",
    "    for sensor in expected_sensors:\n",
    "        view = dataset.select_group_slices(sensor)\n",
    "        field_schema = view.get_field_schema()\n",
    "    \n",
    "        # Check if each optional metadata field exists in the schema\n",
    "        present_fields = []\n",
    "        for key, field in optional_metadata_dict.items():\n",
    "            if field in field_schema:\n",
    "                present_fields.append(key)\n",
    "        \n",
    "        # Return the result\n",
    "        if present_fields:\n",
    "            print(f\"Optional metadata fields present: {present_fields}\")\n",
    "        else:\n",
    "            print(\"No optional metadata fields are present in the dataset schema.\")\n",
    "        present_fields_dict[sensor] = present_fields\n",
    "    return present_fields_dict\n",
    "\n",
    "present_optional_fields = get_present_optional_fields(dataset, optional_metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Verify Sensor Data Presence and Synchronization\n",
    "\n",
    "What to check: \n",
    "\n",
    " Ensure that for each timestamp (or frame index), data from all expected sensors is present and correctly synchronized. For example, if the dataset should have 6 camera images and 1 LiDAR scan per frame, verify none of these modalities are missing. Each sample should contain data from every sensor in the rig (unless the sensor intentionally did not record for that frame). Additionally, confirm that the timestamps of different sensor data in the same frame are either identical or within an acceptable sync tolerance (depending on dataset specification). \n",
    " \n",
    "Why it’s important: \n",
    "\n",
    " Missing sensor data would create a “blind spot” in that frame’s perception. If one camera frame is dropped or out-of-sync, downstream algorithms might make incorrect assumptions. Many datasets ensure sensors are hardware-synchronized (e.g. Wayve’s rig of 5 cameras is time-synchronized at 10 Hz), so a missing or unsynced frame suggests an issue in conversion. Synchronized timestamps ensure that an image and LiDAR from the same sample depict the same moment in time. \n",
    " \n",
    "How to check: \n",
    "\n",
    " Iterate through the dataset and check each sample for the presence of each sensor’s data. If sensor data is stored in subfields (e.g., sample.camera_front, sample.camera_back, sample.lidar), verify those fields are populated. Also, check timestamp consistency across sensors in a frame (if sensors have separate timestamps). For synchronization, one approach is to ensure the difference between any sensor timestamp and the frame’s master timestamp is below a threshold.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sync_tolerance = 100000  # 100 ms tolerance for timestamp sync in microseconds\n",
    "\n",
    "\n",
    "missing_sensor_groups = []\n",
    "for group in dataset.iter_groups():\n",
    "\n",
    "    timestamp_list = []\n",
    "\n",
    "    # Check presence of each sensor and grab timestamp\n",
    "    for sensor in expected_sensors:\n",
    "        sample = group[sensor]\n",
    "        if sample is None:\n",
    "            print(f\"Missing data: {sensor} is not present for sample {group.id}\")\n",
    "            missing_sensor_groups.append((group.id, sensor))\n",
    "        else:\n",
    "            timestamp_list.append(sample[metadata_dict[\"timestamp\"]])\n",
    "\n",
    "    # Check synchronization of timestamps\n",
    "    if len(timestamp_list) > 1:\n",
    "        min_timestamp = min(timestamp_list)\n",
    "        max_timestamp = max(timestamp_list)\n",
    "\n",
    "        if max_timestamp - min_timestamp > max_sync_tolerance:\n",
    "            print(f\"Timestamp sync issue for group {sample.group.id}: \"\n",
    "                  f\"min {min_timestamp}, max {max_timestamp}, \"\n",
    "                  f\"difference {max_timestamp - min_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that each expected sensor is present in every sample. If a sensor data field is missing, it prints a message (indicating a potential conversion issue causing a blind spot). It also checks sensor timestamp vs. frame timestamp; if the difference exceeds max_sync_tolerance, it flags a synchronization issue. In a correctly converted dataset, we expect each frame to have all sensors present and timestamps closely aligned (often exactly the same timestamp for all modalities if the data were captured simultaneously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: LiDAR-to-Image Projection Alignment (Extrinsic Calibration Check)\n",
    "\n",
    "What to check: \n",
    "\n",
    " Project the 3D LiDAR points into the 2D camera image and verify they align with the visual scene (i.e., points fall on the corresponding objects in the image). This checks that the LiDAR–camera extrinsic calibration (and the camera intrinsics) are correctly applied in the conversion. In practice, you overlay the point cloud on the image and see if, for example, points hit the surfaces of vehicles, pedestrians, and road in the image, rather than being offset. \n",
    " \n",
    "Why it’s important: \n",
    "\n",
    " Misalignments between LiDAR and camera indicate calibration errors or coordinate transform issues. If the transformation from LiDAR coordinates to camera coordinates is wrong, the projected points will not correspond to the correct image features. For instance, a misplaced rotation or translation could cause LiDAR points to appear shifted (e.g., hovering beside the actual object). Visualizing LiDAR over images is a common sanity check – misprojections can reveal calibration or time-sync problems. [In research literature](https://www.researchgate.net/figure/Examples-of-projecting-lidar-points-to-images-before-top-row-and-after-bottom-row_fig1_339813627), it’s noted that misalignment of lidar points and image features will be clearly visible if calibration is off or if motion distortion isn’t handled \n",
    " \n",
    "How to check: \n",
    "\n",
    " Use the known calibration parameters to transform LiDAR points to the camera frame and then apply the camera intrinsics to project to pixel coordinates. Then, visualize or statistically verify alignment. Using FiftyOne, one can write a custom loop or use utility functions if available. Below is an example using open3d and opencv to project points onto an image. We assume we have the camera intrinsic matrix K and a 4x4 homogeneous transform T_lidar_to_cam for the extrinsics (from LiDAR frame to camera frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "\n",
    "\n",
    "def transform_points(points, transform):\n",
    "    points_hom = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    \n",
    "    if not np.all(np.isfinite(points_hom)):\n",
    "        raise ValueError(\"Non-finite values in homogeneous points.\")\n",
    "\n",
    "    if not np.all(np.isfinite(transform)):\n",
    "        raise ValueError(\"Non-finite values in transform matrix.\")\n",
    "\n",
    "    # Check max magnitude (overflow-safe)\n",
    "    if np.abs(points_hom).max() > 1e6:\n",
    "        print(\"⚠️ Warning: Unusually large point values detected in transform_points input.\")\n",
    "\n",
    "    result = transform @ points_hom.T  # shape (4, N)\n",
    "\n",
    "    if not np.all(np.isfinite(result)):\n",
    "        print(\"⚠️ Warning: Non-finite values found after matrix multiplication.\")\n",
    "    \n",
    "    return result.T[:, :3]\n",
    "\n",
    "def translate(points, x: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a translation to the point cloud.\n",
    "        :param x: <np.float: 3, 1>. Translation in x, y, z.\n",
    "        \"\"\"\n",
    "        for i in range(3):\n",
    "            points[i, :] = points[i, :] + x[i]\n",
    "\n",
    "        return points\n",
    "\n",
    "def rotate(points, rot_matrix: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a rotation.\n",
    "        :param rot_matrix: <np.float: 3, 3>. Rotation matrix.\n",
    "        \"\"\"\n",
    "        points[:3, :] = np.dot(rot_matrix, points[:3, :])\n",
    "\n",
    "        return points\n",
    "\n",
    "def transform(points, transf_matrix: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a homogeneous transform.\n",
    "        :param transf_matrix: <np.float: 4, 4>. Homogenous transformation matrix.\n",
    "        \"\"\"\n",
    "        points[:3, :] = transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def view_points(points: np.ndarray, view: np.ndarray, normalize: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This is a helper class that maps 3d points to a 2d plane. It can be used to implement both perspective and\n",
    "    orthographic projections. It first applies the dot product between the points and the view. By convention,\n",
    "    the view should be such that the data is projected onto the first 2 axis. It then optionally applies a\n",
    "    normalization along the third dimension.\n",
    "\n",
    "    For a perspective projection the view should be a 3x3 camera matrix, and normalize=True\n",
    "    For an orthographic projection with translation the view is a 3x4 matrix and normalize=False\n",
    "    For an orthographic projection without translation the view is a 3x3 matrix (optionally 3x4 with last columns\n",
    "     all zeros) and normalize=False\n",
    "\n",
    "    :param points: <np.float32: 3, n> Matrix of points, where each point (x, y, z) is along each column.\n",
    "    :param view: <np.float32: n, n>. Defines an arbitrary projection (n <= 4).\n",
    "        The projection should be such that the corners are projected onto the first 2 axis.\n",
    "    :param normalize: Whether to normalize the remaining coordinate (along the third axis).\n",
    "    :return: <np.float32: 3, n>. Mapped point. If normalize=False, the third coordinate is the height.\n",
    "    \"\"\"\n",
    "\n",
    "    assert view.shape[0] <= 4\n",
    "    assert view.shape[1] <= 4\n",
    "    assert points.shape[0] == 3\n",
    "\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "    nbr_points = points.shape[1]\n",
    "\n",
    "    # Do operation in homogenous coordinates.\n",
    "    points = np.concatenate((points, np.ones((1, nbr_points))))\n",
    "    points = np.dot(viewpad, points)\n",
    "    points = points[:3, :]\n",
    "\n",
    "    if normalize:\n",
    "        points = points / points[2:3, :].repeat(3, 0).reshape(3, nbr_points)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def load_point_cloud_pcd(path):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    return np.asarray(pcd.points)\n",
    "\n",
    "def project_lidar_to_image_fiftyone(\n",
    "        img_sample,\n",
    "        pcd_sample,\n",
    "        pcd_path,\n",
    "        min_depth=1.0,\n",
    "        metadata_dict=metadata_dict,\n",
    "        optional_metadata_dict=optional_metadata_dict,\n",
    "        r_and_t_transform = False,\n",
    "        verbose=False):\n",
    "    # Load point cloud\n",
    "    pts = load_point_cloud_pcd(pcd_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Raw point cloud shape:\", pts.shape)\n",
    "        print(\"Any NaNs or infs?\", not np.all(np.isfinite(pts)))\n",
    "\n",
    "    # TO DO\n",
    "    if not r_and_t_transform:\n",
    "        # Load transforms\n",
    "        T_lidar_to_rig = np.array(pcd_sample[metadata_dict[\"T_sensor_rig\"]])  # 4x4\n",
    "        T_rig_to_world_lidar = np.array(pcd_sample[metadata_dict[\"T_rig_world\"]])  # 4x4\n",
    "\n",
    "        T_cam_to_rig = np.array(img_sample[metadata_dict[\"T_sensor_rig\"]]) # 4x4\n",
    "        T_rig_to_world_cam = np.array(img_sample[metadata_dict[\"T_rig_world\"]])  # 4x4\n",
    "\n",
    "\n",
    "        K = np.array(img_sample[\"intrinsics\"])  # 3x3\n",
    "\n",
    "        # Full chain: lidar -> rig -> world -> rig(cam) -> cam\n",
    "        T_world_to_rig_cam = np.linalg.inv(T_rig_to_world_cam)\n",
    "        T_rig_to_cam = np.linalg.inv(T_cam_to_rig)\n",
    "\n",
    "        T_lidar_to_cam = T_rig_to_cam @ T_world_to_rig_cam @ T_rig_to_world_lidar @ T_lidar_to_rig\n",
    "        pts_cam = transform_points(pts, T_lidar_to_cam)\n",
    "\n",
    "    else:\n",
    "\n",
    "        pts = pts.T # shape (3, N)\n",
    "        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "        # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "        pcd_sensor_rotation = pcd_sample[optional_metadata_dict[\"cs_rotation\"]]  \n",
    "        pcd_sensor_translation = pcd_sample[optional_metadata_dict[\"cs_translation\"]]\n",
    "        pts = rotate(pts, Quaternion(pcd_sensor_rotation).rotation_matrix)\n",
    "        pts = translate(pts, np.array(pcd_sensor_translation))\n",
    "\n",
    "        # Second step: transform from ego to the global frame.\n",
    "        pcd_rig_rotation = pcd_sample[optional_metadata_dict[\"ego_rotation\"]]\n",
    "        pcd_rig_translation = pcd_sample[optional_metadata_dict[\"ego_translation\"]]\n",
    "        pts = rotate(pts, Quaternion(pcd_rig_rotation).rotation_matrix)\n",
    "        pts = translate(pts, np.array(pcd_rig_translation))\n",
    "\n",
    "        # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "        img_rig_rotation = img_sample[optional_metadata_dict[\"ego_rotation\"]]\n",
    "        img_rig_translation = img_sample[optional_metadata_dict[\"ego_translation\"]]\n",
    "        pts = translate(pts, -np.array(img_rig_translation))\n",
    "        pts = rotate(pts, Quaternion(img_rig_rotation).rotation_matrix.T)\n",
    "\n",
    "        # Fourth step: transform from ego into the camera.\n",
    "        img_sensor_rotation = img_sample[optional_metadata_dict[\"cs_rotation\"]]  \n",
    "        img_sensor_translation = img_sample[optional_metadata_dict[\"cs_translation\"]]\n",
    "        pts = translate(pts, -np.array(img_sensor_translation))\n",
    "        pts = rotate(pts, Quaternion(img_sensor_rotation).rotation_matrix.T)\n",
    "\n",
    "        # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pts[2, :]\n",
    "\n",
    "        coloring = depths\n",
    "\n",
    "        points = view_points(pts[:3, :], np.array(img_sample['intrinsics']), normalize=True)\n",
    "\n",
    "\n",
    "\n",
    "        im = Image.open(img_sample.filepath)\n",
    "        mask = np.ones(depths.shape[0], dtype=bool)\n",
    "        mask = np.logical_and(mask, depths > min_depth)\n",
    "        mask = np.logical_and(mask, points[0, :] > 1)\n",
    "        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "        mask = np.logical_and(mask, points[1, :] > 1)\n",
    "        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "        points = points[:, mask]\n",
    "        coloring = coloring[mask]\n",
    "\n",
    "\n",
    "        return points, coloring, im\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Camera-frame Z stats:\")\n",
    "        print(\"  Min Z:\", np.min(pts_cam[:, 2]))\n",
    "        print(\"  Max Z:\", np.max(pts_cam[:, 2]))\n",
    "        print(\"  Any Z ≤ 0?\", np.any(pts_cam[:, 2] <= 0))\n",
    "        print(\"  Any NaN or inf?\", not np.all(np.isfinite(pts_cam)))\n",
    "\n",
    "    # Filter points behind camera\n",
    "    mask = pts_cam[:, 2] > min_depth\n",
    "    pts_cam = pts_cam[mask]\n",
    "\n",
    "    # Project to image\n",
    "    projected = K @ pts_cam.T\n",
    "    projected = projected.T\n",
    "    # Z = depth (3rd column)\n",
    "    z = projected[:, 2]\n",
    "    valid = np.isfinite(z) & (z > min_depth)\n",
    "\n",
    "    projected = projected[valid]\n",
    "    z = z[valid][:, None]\n",
    "\n",
    "    pts_2d = projected[:, :2] / z\n",
    "\n",
    "    return pts_2d, pts_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.threed as fotd\n",
    "\n",
    "for group in dataset.iter_groups():\n",
    "    for img_sensor in img_sensors:\n",
    "        img_sample = group[img_sensor]\n",
    "        pcd_sample = group[\"3D\"]\n",
    "\n",
    "        if img_sample is None or pcd_sample is None:\n",
    "            print(f\"Missing data: {img_sensor} or 3D for sample {group.id}\")\n",
    "            continue\n",
    "\n",
    "        fo3d_path = pcd_sample.filepath\n",
    "        pcd_paths = fo.Scene().from_fo3d(fo3d_path).get_asset_paths()\n",
    "        pcd_path = next(path for path in pcd_paths if \"LIDAR_TOP\" in path)\n",
    "\n",
    "        pts_2d, coloring,im = project_lidar_to_image_fiftyone(\n",
    "            img_sample,\n",
    "            pcd_sample,\n",
    "            pcd_path,\n",
    "            metadata_dict=metadata_dict,\n",
    "            r_and_t_transform=True\n",
    "        )\n",
    "\n",
    "        if pts_2d.size == 0:\n",
    "            print(f\"No valid points projected for {img_sensor} in sample {group.id}\")\n",
    "            continue\n",
    "\n",
    "        img_sample[\"lidar_points\"] = fo.Keypoints(\n",
    "        keypoints = [\n",
    "            fo.Keypoint(\n",
    "                label=\"LIDAR_TOP\",\n",
    "                points = list(zip(pts_2d.T[:, 0]/img_sample.metadata.width, pts_2d.T[:, 1]/img_sample.metadata.height)),\n",
    "                colors = list(coloring)\n",
    "            )\n",
    "            ]\n",
    "        )\n",
    "        img_sample.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driving_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
