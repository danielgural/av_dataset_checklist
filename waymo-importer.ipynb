{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Notebook Overview\n",
    "This notebook reads and parses frames from the Waymo Open Dataset using TensorFlow and Protocol Buffers. It then extracts and displays image data and LIDAR information from each frame for visualization or further processing.\n",
    "\n",
    "## üì¶  Imports and Protobuf Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This cell imports:\n",
    "- TensorFlow: to load TFRecord data.\n",
    "- Waymo's `dataset_pb2` module: provides access to Protocol Buffers for parsing Waymo frames.\n",
    "- PIL and NumPy: for handling and converting image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import click\n",
    "import imageio as imageio_v1\n",
    "\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Union\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from waymo_open_dataset import dataset_pb2, label_pb2\n",
    "from waymo_open_dataset.utils import frame_utils\n",
    "from google.protobuf import json_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Waymo TFRecord Dataset\n",
    "\n",
    "Here we load a Waymo `.tfrecord` file using `TFRecordDataset`. The `compression_type=''` indicates the file is uncompressed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WaymoProto2SemanticLabel = {\n",
    "    label_pb2.Label.Type.TYPE_UNKNOWN: \"Unknown\",\n",
    "    label_pb2.Label.Type.TYPE_VEHICLE: \"Car\",\n",
    "    label_pb2.Label.Type.TYPE_PEDESTRIAN: \"Pedestrian\",\n",
    "    label_pb2.Label.Type.TYPE_SIGN: \"Sign\",\n",
    "    label_pb2.Label.Type.TYPE_CYCLIST: \"Cyclist\",\n",
    "}\n",
    "\n",
    "CameraNames = ['front', 'front_left', 'front_right', 'side_left', 'side_right']\n",
    "\n",
    "SourceFps = 10 # waymo's recording fps\n",
    "TargetFps = 30 # cosmos's expected fps\n",
    "IndexScaleRatio = int(TargetFps / SourceFps)\n",
    "\n",
    "if int(tf.__version__.split(\".\")[0]) < 2:\n",
    "    tf.enable_eager_execution()\n",
    "\n",
    "def get_camera_name(name_int) -> str:\n",
    "    return dataset_pb2.CameraName.Name.Name(name_int)\n",
    "\n",
    "\n",
    "def get_lidar_name(name_int) -> str:\n",
    "    return dataset_pb2.LaserName.Name.Name(name_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waymo_tfrecord_root = \"/home/dan/development/data_sanity/data-sample\"\n",
    "all_filenames = list(Path(waymo_tfrecord_root).glob(\"*.tfrecord\"))\n",
    "print(f\"Found {len(all_filenames)} tfrecords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéûÔ∏è Convert Waymo Images to Video Frames\n",
    "\n",
    "This function reads all frames from a Waymo TFRecord dataset and extracts camera images. It organizes the images by camera and saves them as individual JPEG files. These can later be stitched into a video or used independently for analysis or visualization.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Function:** `convert_waymo_image(...)`\n",
    "\n",
    "- **Parameters:**\n",
    "  - `output_root (Path)`: Directory where the output images will be saved.\n",
    "  - `clip_id (str)`: Identifier to prefix output filenames.\n",
    "  - `dataset (tf.data.TFRecordDataset)`: Parsed Waymo TFRecord dataset.\n",
    "  - `single_camera (bool)`: If `True`, only the 'front' camera is processed.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Workflow:**\n",
    "1. ‚úÖ **Initialize storage** for each camera‚Äôs image sequence using `CameraNames`.\n",
    "2. üîÑ **Iterate through each frame** in the dataset:\n",
    "   - Parse the serialized frame data into a `Frame` protobuf.\n",
    "   - Extract each image, decode it from JPEG, convert to NumPy, and append to the corresponding camera's list.\n",
    "3. üíæ **Save extracted images**:\n",
    "   - If `single_camera=True`, skip all but the 'front' camera.\n",
    "   - Write each image to disk in the format:  \n",
    "     `pinhole_<camera_name>/<clip_id>_<frame_index>.jpg`.\n",
    "   - Create output directories as needed.\n",
    "\n",
    "---\n",
    "\n",
    "This function assumes Waymo cameras operate at **10 Hz**, meaning each second of footage yields 10 frames per camera.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waymo_image(output_root: Path, clip_id: str, dataset: tf.data.TFRecordDataset, single_camera: bool = False  ):\n",
    "    \"\"\"\n",
    "    read all frames and convert the images to video format.\n",
    "    \"\"\"\n",
    "    cprint('reading image and converting to video, this may take a while...', 'yellow')\n",
    "\n",
    "    camera_name_to_image_sequence = {}\n",
    "    for camera_name in CameraNames:\n",
    "        camera_name_to_image_sequence[camera_name] = []\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "        for image_data in frame.images:\n",
    "            camera_name = get_camera_name(image_data.name).lower()\n",
    "            image_data_bytes = image_data.image\n",
    "            image_data_tf_tensor = tf.image.decode_jpeg(image_data_bytes)\n",
    "            image_data_numpy = image_data_tf_tensor.numpy()\n",
    "            camera_name_to_image_sequence[camera_name].append(image_data_numpy)\n",
    "    \n",
    "    for camera_name, image_sequence in camera_name_to_image_sequence.items():\n",
    "        # waymo is recorded at 10 Hz\n",
    "        if single_camera and camera_name != 'front':\n",
    "           continue\n",
    "\n",
    "        for i,image in enumerate(image_sequence):\n",
    "            output_image_path = (output_root / f\"pinhole_{camera_name}\" / f'{clip_id}_{i}.jpg')\n",
    "            output_image_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            imageio_v1.imwrite(output_image_path, image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Convert Waymo LIDAR to Cosmos Format\n",
    "\n",
    "This function processes each frame in a Waymo TFRecord dataset to extract LIDAR point clouds and saves them in `.pcd` format using Open3D.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Function:** `convert_waymo_lidar(...)`\n",
    "\n",
    "- **Parameters:**\n",
    "  - `output_root (Path)`: Directory where the LIDAR point clouds will be saved.\n",
    "  - `clip_id (str)`: Identifier used to prefix output files.\n",
    "  - `dataset (tf.data.TFRecordDataset)`: Waymo TFRecord dataset.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def convert_waymo_lidar(output_root: Path, clip_id: str, dataset: tf.data.TFRecordDataset):\n",
    "    \"\"\"\n",
    "    read all frames and convert the lidar\n",
    "\n",
    "    \"\"\"\n",
    "    sample = {'__key__': clip_id}\n",
    "    cprint('reading lidar and converting to cosmos format, this may take a few minutes...', 'yellow')\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "        vehicle_to_world = np.array(frame.pose.transform).reshape((4, 4))\n",
    "        \n",
    "        range_images, camera_projections,  _, range_image_top_pose = \\\n",
    "            frame_utils.parse_range_image_and_camera_projection(frame)\n",
    "\n",
    "        points, _ = frame_utils.convert_range_image_to_point_cloud(\n",
    "            frame, range_images, camera_projections, range_image_top_pose\n",
    "        )\n",
    "        points_ri2, _ = frame_utils.convert_range_image_to_point_cloud(\n",
    "            frame, range_images, camera_projections, range_image_top_pose, ri_index=1\n",
    "        )\n",
    "\n",
    "        # 3d points in vehicle frame. \n",
    "        points = [np.concatenate([p1, p2]) for p1, p2 in zip(points, points_ri2)] \n",
    "\n",
    "        lidar_ids = [calib.name for calib in frame.context.laser_calibrations]\n",
    "        lidar_ids.sort()\n",
    "\n",
    "        for lidar_id, lidar_points in zip(lidar_ids, points):\n",
    "            lidar_name = get_lidar_name(lidar_id)\n",
    "\n",
    "            if lidar_name == 'TOP':\n",
    "                # Convert lidar points to Open3D PointCloud format\n",
    "                pcd = o3d.geometry.PointCloud()\n",
    "                pcd.points = o3d.utility.Vector3dVector(lidar_points)\n",
    "\n",
    "                # Define the output path for the PCD file\n",
    "                pcd_output_path = output_root / 'lidar_pcd' / f\"{clip_id}_{frame_idx * IndexScaleRatio}.pcd\"\n",
    "                pcd_output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                # Save the PointCloud to a PCD file\n",
    "                o3d.io.write_point_cloud(str(pcd_output_path), pcd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è Waymo Dataset Initialization and Configuration\n",
    "\n",
    "This block sets up paths, parameters, and the data loading pipeline needed to process a Waymo TFRecord file for conversion into image and LIDAR formats.\n",
    "\n",
    "---\n",
    "\n",
    "#### **üì∑ Camera Setup**\n",
    "Defines the list of camera names present in the Waymo dataset:\n",
    "```python\n",
    "CameraNames = ['front', 'front_left', 'front_right', 'side_left', 'side_right']\n",
    "```\n",
    "\n",
    "### Unpacking the TFRecord\n",
    "This block also will unpack a `tfrecord` into `jpg`s and `pcd`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CameraNames = ['front', 'front_left', 'front_right', 'side_left', 'side_right']\n",
    "\n",
    "SourceFps = 10 # waymo's recording fps\n",
    "TargetFps = 30 # cosmos's expected fps\n",
    "IndexScaleRatio = int(TargetFps / SourceFps)\n",
    "\n",
    "waymo_tfrecord_root = \"/home/dan/development/data_sanity/data-sample/\"\n",
    "all_filenames = list(Path(waymo_tfrecord_root).glob(\"*.tfrecord\"))\n",
    "print(f\"Found {len(all_filenames)} tfrecords\")\n",
    "\n",
    "single_camera = False\n",
    "\n",
    "waymo_tfrecord_filename = all_filenames[0].name\n",
    "output_data_path = \"/home/dan/development/data_sanity/data-sample/fiftyone\" \n",
    "waymo_tfrecord_path = Path(waymo_tfrecord_root + waymo_tfrecord_filename)\n",
    "clip_id = waymo_tfrecord_path.stem.lstrip('segment-').rstrip('_with_camera_labels')\n",
    "output_data_path = Path(output_data_path)\n",
    "\n",
    "if not waymo_tfrecord_path.exists():\n",
    "    raise FileNotFoundError(f\"Waymo tfrecord file not found: {waymo_tfrecord_path}\")\n",
    "\n",
    "\n",
    "tf_dataset = tf.data.TFRecordDataset(waymo_tfrecord_path, compression_type=\"\")\n",
    "\n",
    "\n",
    "convert_waymo_image(output_data_path, clip_id, tf_dataset, single_camera)\n",
    "convert_waymo_lidar(output_data_path, clip_id, tf_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating FiftyOne Dataset\n",
    "\n",
    "This code initializes a FiftyOne dataset named `\"Waymo-Open-Dataset-Test\"` and populates it with grouped samples composed of camera images and LIDAR point clouds extracted from a Waymo TFRecord export. It begins by loading all front-facing camera images from the output directory, sorting them by frame index. For each image, it constructs a sample group that may include additional camera views (if `single_camera` is `False`) and a corresponding `.pcd` LIDAR file if available.\n",
    "\n",
    "Each image is added as an individual `fo.Sample`, assigned a sensor name and grouped by frame. If LIDAR data is present, it is wrapped in a `fo.Scene` object, written to disk as a `.fo3d` file, and added as a 3D sample to the group. All samples are appended to a list and finally registered with the FiftyOne dataset. The result is a synchronized, multi-sensor dataset where each frame includes images from multiple cameras and an optional LIDAR scene, viewable and queryable in FiftyOne.\n",
    "\n",
    "See [FiftyOne Grouped Datasets](https://docs.voxel51.com/user_guide/groups.html) for more info!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "fo_dataset = fo.Dataset(name=\"Waymo-Open-Dataset-Test\", persistent=True, overwrite=True)\n",
    "\n",
    "image_dir = output_data_path / \"pinhole_front\"\n",
    "image_files = list(image_dir.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(image_files)} images in {image_dir}\")\n",
    "image_files.sort(key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "samples = []\n",
    "for image_file in image_files:\n",
    "    \n",
    "    group = fo.Group()\n",
    "    frame = image_file.stem.split('_')[-1]\n",
    "    clip_id = \"_\".join(image_file.stem.split('_')[:5])\n",
    "    sensors = {}\n",
    "    sensors['pinhole_front'] = {\n",
    "        'file': image_file,\n",
    "        'frame': int(frame),\n",
    "        'clip_id': clip_id,\n",
    "        'sensor_name': 'front',\n",
    "        'modality': 'image'\n",
    "    }\n",
    "    if not single_camera:\n",
    "        for camera_name in CameraNames:\n",
    "            if camera_name != 'front':\n",
    "                other_camera_dir = output_data_path / f\"pinhole_{camera_name}/\"\n",
    "                other_camera_file = str(other_camera_dir) + f\"/{clip_id}_{frame}.jpg\"\n",
    "                \n",
    "                if not Path(other_camera_file).exists():\n",
    "                    print(f\"File not found: {other_camera_file}\")\n",
    "                    continue\n",
    "                else:\n",
    "                    sensors[f'pinhole_{camera_name}'] = {\n",
    "                        'file': other_camera_file,\n",
    "                        'frame': int(frame),\n",
    "                        'clip_id': clip_id,\n",
    "                        'sensor_name': camera_name,\n",
    "                        'modality': 'image'\n",
    "                    }\n",
    "    lidar_dir = output_data_path / f\"lidar_pcd/\"\n",
    "    lidar_file = str(lidar_dir) + f\"/{clip_id}_{frame}.pcd\"\n",
    "    if Path(lidar_file).exists():\n",
    "        sensors['lidar'] = {\n",
    "            'file': lidar_file,\n",
    "            'frame': int(frame),\n",
    "            'clip_id': clip_id,\n",
    "            'sensor_name': 'lidar_top',\n",
    "            'modality': 'pointcloud'\n",
    "        }\n",
    "\n",
    "    for sensor_name, sensor_data in sensors.items():\n",
    "        fo3d_scene = fo.Scene(camera=fo.PerspectiveCamera(up=\"Z\"))\n",
    "        fo3d_output_dir = output_data_path / f\"fo3d/\"\n",
    "        fo3d_filepath = str(fo3d_output_dir) + f\"/{clip_id}_{frame}.fo3d\"\n",
    "        Path(fo3d_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        if sensor_data[\"modality\"] == \"image\":\n",
    "            sample = fo.Sample(\n",
    "                filepath=sensor_data['file'],\n",
    "                frame=sensor_data['frame'],\n",
    "                clip_id=sensor_data['clip_id'],\n",
    "                sensor_name=sensor_data['sensor_name'],\n",
    "                group=group.element(sensor_name)\n",
    "            )\n",
    "            samples.append(sample)\n",
    "        else:\n",
    "            fo3d_scene.add(\n",
    "                    fo.PointCloud(\n",
    "                        name=sensor_name,\n",
    "                        pcd_path=sensor_data['file'],\n",
    "                        flag_for_projection=sensor_name == \"lidar_top\",\n",
    "                    )\n",
    "                )\n",
    "    fo3d_scene.write(fo3d_filepath)\n",
    "    fo3d_sample = fo.Sample(filepath=fo3d_filepath, group=group.element(\"3D\"))\n",
    "    fo3d_sample[\"clip_id\"] = sensor_data['clip_id']\n",
    "    fo3d_sample[\"frame\"] = sensor_data['frame']\n",
    "    fo3d_sample[\"sensor_name\"] = \"lidar_top\"\n",
    "    samples.append(fo3d_sample)\n",
    "\n",
    "fo_dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(fo_dataset, port=2830)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels and Metadata\n",
    "\n",
    "Now that we have gotten through the core media in jpgs and pcds, its time to add the rest! Note that some of this is still a work in progress for one reason or another. Here is a comprehensive list of what exists here:\n",
    "\n",
    "- `convert_waymo_hdmap` (works, but bugged in latest FO release )\n",
    "- `convert_waymo_intrinsics` (works!)\n",
    "- `convert_waymo_pose` (works!)\n",
    "- `convert_waymo_timestamp` (works!)\n",
    "- `convert_waymo_3dbbox` (bugged)\n",
    "- `convert_waymo_2dbbox` (works!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waymo_hdmap(sample, clip_id: str, dataset: tf.data.TFRecordDataset, ):\n",
    "    \"\"\"\n",
    "    read the first frame and convert the hdmap to wds format.\n",
    "    \"\"\"\n",
    "    def hump_to_underline(hump_str):\n",
    "        import re\n",
    "        return re.sub(r'([a-z])([A-Z])', r'\\1_\\2', hump_str).lower()\n",
    "\n",
    "    hdmap_names_polyline = [\"lane\", \"road_line\", \"road_edge\"]\n",
    "    hdmap_names_polygon = [\"crosswalk\", \"speed_bump\", \"driveway\"]\n",
    "    \n",
    "    hdmap_name_to_data = {}\n",
    "    for hdmap_name in hdmap_names_polyline + hdmap_names_polygon:\n",
    "        hdmap_name_to_data[hump_to_underline(hdmap_name)] = []\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        if frame_idx == sample.frame:\n",
    "            frame = dataset_pb2.Frame()\n",
    "            frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "            map_features_list = json_format.MessageToDict(frame)['mapFeatures']\n",
    "\n",
    "            for hdmap_content in map_features_list:\n",
    "                hdmap_name = list(hdmap_content.keys())\n",
    "                hdmap_name.remove(\"id\")\n",
    "                hdmap_name = hdmap_name[0]\n",
    "                hdmap_name_lower = hump_to_underline(hdmap_name)\n",
    "\n",
    "                hdmap_data = hdmap_content[hdmap_name]\n",
    "                if hdmap_name_lower in hdmap_names_polyline:\n",
    "                    hdmap_data = hdmap_data['polyline']\n",
    "                    polyline = [[point['x'], point['y'], point['z']] for point in hdmap_data]\n",
    "                    hdmap_name_to_data[hdmap_name_lower].append(polyline)\n",
    "                elif hdmap_name_lower in hdmap_names_polygon:\n",
    "                    hdmap_data = hdmap_data['polygon']\n",
    "                    polygon = [[point['x'], point['y'], point['z']] for point in hdmap_data]\n",
    "                    hdmap_name_to_data[hdmap_name_lower].append(polygon)\n",
    "                else:\n",
    "                    print(f\"Unkown hdmap item name: {hdmap_name}Ôºåskip this item\")\n",
    "\n",
    "\n",
    "    # convert to cosmos's name convention for easier processing\n",
    "    hdmap_name_to_cosmos = {\n",
    "        'lane': 'lanes',\n",
    "        'road_line': 'lanelines',\n",
    "        'road_edge': 'road_boundaries',\n",
    "        'crosswalk': 'crosswalks',\n",
    "        'speed_bump': None,\n",
    "        'driveway': None\n",
    "    }\n",
    "\n",
    "    polylines = []\n",
    "    for hdmap_name, hdmap_data in hdmap_name_to_data.items():\n",
    "        hdmap_name_in_cosmos = hdmap_name_to_cosmos[hdmap_name]\n",
    "        if hdmap_name_in_cosmos is None:\n",
    "            continue\n",
    "\n",
    "        if hdmap_name in hdmap_names_polyline:\n",
    "            vertex_indicator = 'polyline3d'\n",
    "        else:\n",
    "            vertex_indicator = 'surface'\n",
    "\n",
    "\n",
    "        for each_polyline_or_polygon in hdmap_data:\n",
    "\n",
    "            if each_polyline_or_polygon is None or len(each_polyline_or_polygon) == 0:\n",
    "                print(f\"Skipping empty {hdmap_name} data\")\n",
    "                continue\n",
    "\n",
    "            polyline = fo.Polyline(label=hdmap_name, points3d=each_polyline_or_polygon)\n",
    "            return each_polyline_or_polygon\n",
    "            polylines.append(polyline)\n",
    "    \n",
    "    sample[\"polylines\"] = fo.Polylines(polylines=polylines)\n",
    "    sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waymo_intrinsics(sample, clip_id: str, dataset: tf.data.TFRecordDataset,):\n",
    "    \"\"\"\n",
    "    read the first frame and convert the intrinsics to wds format\n",
    "\n",
    "    Minimal required format:\n",
    "        sample['pinhole_intrinsic.{camera_name}.npy'] = np.ndarray with shape (4, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "         if frame_idx == sample.frame:\n",
    "            \n",
    "            frame = dataset_pb2.Frame()\n",
    "            frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "            for camera_calib in frame.context.camera_calibrations:\n",
    "                camera_name = get_camera_name(camera_calib.name).lower()\n",
    "                if camera_name == sample.sensor_name:\n",
    "                    intrinsic = camera_calib.intrinsic\n",
    "                    fx, fy, cx, cy = intrinsic[:4]\n",
    "                    w, h = camera_calib.width, camera_calib.height\n",
    "\n",
    "                    sample[\"camera_intrinsics\"] = \\\n",
    "                        np.array([fx, fy, cx, cy, w, h])\n",
    "                    sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "def interpolate_pose(prev_pose, next_pose, t):\n",
    "    \"\"\"\n",
    "    new pose = (1 - t) * prev_pose + t * next_pose.\n",
    "    - linear interpolation for translation\n",
    "    - slerp interpolation for rotation\n",
    "\n",
    "    Args:\n",
    "        prev_pose: np.ndarray, shape (4, 4), dtype=np.float32, previous pose\n",
    "        next_pose: np.ndarray, shape (4, 4), dtype=np.float32, next pose\n",
    "        t: float, interpolation factor\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray, shape (4, 4), dtype=np.float32, interpolated pose\n",
    "\n",
    "    Note:\n",
    "        if input is list, also return list.\n",
    "    \"\"\"\n",
    "    input_is_list = isinstance(prev_pose, list)\n",
    "    prev_pose = np.array(prev_pose)\n",
    "    next_pose = np.array(next_pose)\n",
    "\n",
    "    prev_translation = prev_pose[:3, 3]\n",
    "    next_translation = next_pose[:3, 3]\n",
    "    translation = (1 - t) * prev_translation + t * next_translation\n",
    "\n",
    "    prev_rotation = R.from_matrix(prev_pose[:3, :3])\n",
    "    next_rotation = R.from_matrix(next_pose[:3, :3])\n",
    "    \n",
    "    times = [0, 1]\n",
    "    rotations = R.from_quat([prev_rotation.as_quat(), next_rotation.as_quat()])\n",
    "    rotation = Slerp(times, rotations)(t)\n",
    "\n",
    "    new_pose = np.eye(4)\n",
    "    new_pose[:3, :3] = rotation.as_matrix()\n",
    "    new_pose[:3, 3] = translation\n",
    "\n",
    "    if input_is_list:\n",
    "        return new_pose.tolist()\n",
    "    else:\n",
    "        return new_pose\n",
    "    \n",
    "def convert_waymo_pose(fo_dataset, clip_id: str, dataset: tf.data.TFRecordDataset):\n",
    "    \"\"\"\n",
    "    read all frames and convert the pose to wds format. interpolate the pose to the target fps\n",
    "\n",
    "    Minimal required format:\n",
    "        sample_camera_to_world['{frame_idx:06d}.pose.{camera_name}.npy'] = np.ndarray with shape (4, 4). opencv convention\n",
    "        sample_vehicle_to_world['{frame_idx:06d}.vehicle_pose.npy'] = np.ndarray with shape (4, 4). flu convention\n",
    "    \"\"\"\n",
    "\n",
    "    camera_name_to_camera_to_vehicle = {}\n",
    "    \n",
    "\n",
    "    # get camera_to_vehicle from the first frame\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "        for camera_calib in frame.context.camera_calibrations:\n",
    "            camera_name = get_camera_name(camera_calib.name).lower()\n",
    "            camera_to_vehicle = np.array(camera_calib.extrinsic.transform).reshape((4, 4)) # FLU convention\n",
    "            camera_name_to_camera_to_vehicle[camera_name] = camera_to_vehicle\n",
    "\n",
    "        # only process the first frame\n",
    "        break\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "\n",
    "        group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "        sample = group[\"3D\"]\n",
    "        sample = fo_dataset[sample.filepath]\n",
    "        sample[\"vehicle_to_world\"] = np.array(frame.pose.transform).reshape((4, 4))\n",
    "        sample.save()\n",
    "\n",
    "        for image_data in frame.images:\n",
    "            camera_name = get_camera_name(image_data.name).lower()\n",
    "            sample = group[\"pinhole_\" + camera_name]\n",
    "            sample[\"vehicle_to_world\"] = np.array(image_data.pose.transform).reshape((4, 4))\n",
    "            sample[\"camera_to_vehicle\"] = camera_name_to_camera_to_vehicle[camera_name]\n",
    "            sample[\"camera_to_world\"] = sample[\"vehicle_to_world\"] @ camera_to_vehicle # FLU convention\n",
    "            sample[\"camera_to_world_opencv\"] = np.concatenate(\n",
    "                [-sample[\"camera_to_world\"][:, 1:2], -sample[\"camera_to_world\"][:, 2:3], sample[\"camera_to_world\"][:, 0:1], sample[\"camera_to_world\"][:, 3:4]],\n",
    "                axis=1\n",
    "            )\n",
    "            sample.save()\n",
    "\n",
    "\n",
    "    # interpolate the pose to the target fps\n",
    "    # source index: 0,    1,    2,    3, ..., 10\n",
    "    # target index: 0,1,2,3,4,5,6,7,8,9, ..., 30,31,32\n",
    "    max_target_frame_idx = frame_idx * IndexScaleRatio\n",
    "\n",
    "    # interpolate the vehicle pose to the target fps\n",
    "    for target_frame_idx in range(max_target_frame_idx):\n",
    "        group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "        if group[\"pinhole_\" + camera_name][\"vehicle_to_world\"] is  None:\n",
    "            nearest_prev_frame_idx = target_frame_idx // IndexScaleRatio * IndexScaleRatio\n",
    "            nearest_prev_frame_pose = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==nearest_prev_frame_idx).first().group.id)[\"pinhole_\" + camera_name][\"vehicle_to_world\"]\n",
    "            nearest_next_frame_idx = (target_frame_idx // IndexScaleRatio + 1) * IndexScaleRatio\n",
    "            nearest_next_frame_pose = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==nearest_next_frame_idx).first().group.id)[\"pinhole_\" + camera_name][\"vehicle_to_world\"]\n",
    "            \n",
    "            int_pose = \\\n",
    "                interpolate_pose(nearest_prev_frame_pose, nearest_next_frame_pose, (target_frame_idx - nearest_prev_frame_idx) / IndexScaleRatio)\n",
    "            for _, sample in group.items():\n",
    "                sample[\"vehicle_to_world\"] = int_pose\n",
    "                sample.save()\n",
    "\n",
    "\n",
    "\n",
    "    # interpolate the camera pose to the target fps\n",
    "    for camera_name in CameraNames:\n",
    "        for target_frame_idx in range(max_target_frame_idx):\n",
    "            group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "            if group[\"pinhole_\" + camera_name][\"camera_to_world\"] is  None:\n",
    "                nearest_prev_frame_idx = target_frame_idx // IndexScaleRatio * IndexScaleRatio\n",
    "                nearest_prev_frame_pose = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==nearest_prev_frame_idx).first().group.id)[\"pinhole_\" + camera_name][\"camera_to_world\"]\n",
    "                nearest_next_frame_idx = (target_frame_idx // IndexScaleRatio + 1) * IndexScaleRatio\n",
    "                nearest_next_frame_pose = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==nearest_next_frame_idx).first().group.id)[\"pinhole_\" + camera_name][\"camera_to_world\"]\n",
    "                \n",
    "                int_pose = \\\n",
    "                    interpolate_pose(nearest_prev_frame_pose, nearest_next_frame_pose, (target_frame_idx - nearest_prev_frame_idx) / IndexScaleRatio)\n",
    "                sample = group[\"pinhole_\" + camera_name]\n",
    "                sample[\"camera_to_world\"] = int_pose\n",
    "                sample.save()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waymo_timestamp(fo_dataset, clip_id: str, dataset: tf.data.TFRecordDataset):\n",
    "    \"\"\"\n",
    "    read all frames and convert the timestamp to wds format.\n",
    "    \"\"\"\n",
    "    \n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "        timestamp_micros = frame.timestamp_micros\n",
    "        group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "        for _, sample in group.items():\n",
    "                sample[\"timestamp\"] = timestamp_micros\n",
    "                sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "def convert_waymo_3dbbox(fo_dataset, clip_id: str, dataset: tf.data.TFRecordDataset):\n",
    "    \"\"\"\n",
    "    read all frames and convert the 3dbbox \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    min_moving_speed = 0.2\n",
    "\n",
    "    valid_bbox_types = [\n",
    "        label_pb2.Label.Type.TYPE_VEHICLE,\n",
    "        label_pb2.Label.Type.TYPE_PEDESTRIAN,\n",
    "        label_pb2.Label.Type.TYPE_CYCLIST\n",
    "    ]\n",
    "\n",
    "    for frame_idx, data in enumerate(dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "        group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "\n",
    "        vehicle_to_world = np.array(frame.pose.transform).reshape((4, 4))\n",
    "\n",
    "        detections = []\n",
    "        for label in frame.laser_labels:\n",
    "            if label.type not in valid_bbox_types:\n",
    "                continue\n",
    "\n",
    "            if not label.camera_synced_box.ByteSize():\n",
    "                continue\n",
    "\n",
    "            object_id = label.id\n",
    "            object_type = WaymoProto2SemanticLabel[label.type]\n",
    "\n",
    "            center_in_vehicle = np.array([label.camera_synced_box.center_x, label.camera_synced_box.center_y, label.camera_synced_box.center_z, 1]).reshape((4, 1))\n",
    "            center_in_world = vehicle_to_world @ center_in_vehicle\n",
    "            heading = label.camera_synced_box.heading\n",
    "            rotation_in_vehicle = R.from_euler(\"xyz\", [0, 0, heading], degrees=False).as_matrix()\n",
    "            rotation_in_world = vehicle_to_world[:3, :3] @ rotation_in_vehicle\n",
    "\n",
    "            object_to_world = np.eye(4)\n",
    "            object_to_world[:3, :3] = rotation_in_world\n",
    "            object_to_world[:3, 3] = center_in_world.flatten()[:3]\n",
    "\n",
    "            object_lwh = np.array([label.camera_synced_box.length, label.camera_synced_box.width, label.camera_synced_box.height])\n",
    "            \n",
    "            speed = np.sqrt(label.metadata.speed_x**2 + label.metadata.speed_y**2 + label.metadata.speed_z**2)\n",
    "            object_is_moving = bool(speed > min_moving_speed)\n",
    "            center_in_world = center_in_world.flatten()[:3]\n",
    "            print(center_in_vehicle.flatten()[:3])\n",
    "            detection = fo.Detection(\n",
    "                label=object_type,\n",
    "                location=center_in_world,\n",
    "                dimensions=object_lwh,\n",
    "                rotation=R.from_matrix(rotation_in_vehicle).as_euler('xyz', degrees=False),\n",
    "                )\n",
    "            detections.append(detection)\n",
    "        \n",
    "        sample = group[\"3D\"]\n",
    "        detections.append(fo.Detection(\n",
    "            label=\"vehicle\",\n",
    "            location=[0,0,0],\n",
    "            dimensions=[3,3,3],\n",
    "            rotation=[0, 0, 0],\n",
    "        ))\n",
    "        sample[\"detections\"] = fo.Detections(detections=detections)\n",
    "        sample.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_waymo_2dbbox(fo_dataset, tf_dataset: tf.data.TFRecordDataset):\n",
    "    for frame_idx, data in enumerate(tf_dataset):\n",
    "        frame = dataset_pb2.Frame()\n",
    "        frame.ParseFromString(bytes(data.numpy()))\n",
    "        group = fo_dataset.get_group(fo_dataset.match(F(\"frame\")==frame_idx).first().group.id)\n",
    "        for camera_labels in frame.camera_labels:\n",
    "        # Ignore camera labels that do not correspond to this camera.\n",
    "            sample = group[\"pinhole_\" + get_camera_name(camera_labels.name).lower()]\n",
    "            detections = []\n",
    "            for label in camera_labels.labels:\n",
    "                x = (label.box.center_x - 0.5 * label.box.length)/sample.metadata.width\n",
    "                y = (label.box.center_y - 0.5 * label.box.width)/ sample.metadata.height\n",
    "                width = label.box.length / sample.metadata.width\n",
    "                height = label.box.width / sample.metadata.height\n",
    "                label = WaymoProto2SemanticLabel[label.type]\n",
    "                det = fo.Detection(\n",
    "                    label=label,\n",
    "                    bounding_box=[x, y, width, height],\n",
    "                )\n",
    "                detections.append(det)\n",
    "            sample[\"2d_detections\"] = fo.Detections(detections=detections)\n",
    "            sample.save()\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to compute metadata first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset.compute_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_waymo_2dbbox(fo_dataset, tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_view = fo_dataset.select_group_slices(media_type=\"image\")\n",
    "for sample in image_view:\n",
    "    convert_waymo_intrinsics(sample, clip_id, tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_waymo_pose(fo_dataset, clip_id, tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_waymo_timestamp(fo_dataset, clip_id, tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_waymo_3dbbox(fo_dataset, clip_id, tf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset.export(\n",
    "    export_dir=\"fo_waymo_dataset\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waymo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
