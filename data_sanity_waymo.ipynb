{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sanity Checklist for Autonomous Vehicle Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autonomous vehicle datasets (e.g., nuScenes, Waymo, Wayve) often require conversion into a common format. After conversion, it’s crucial to verify the integrity and consistency of the data. Below is a comprehensive sanity-check checklist for multimodal autonomous driving data (images, LiDAR, etc.), with explanations and example code (using Python and the FiftyOne tool along with other libraries) for each check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Your FiftyOne Dataset In"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=\"fo_waymo_dataset\",\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=\"waymo-sample\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Ensure Metadata Completeness and Consistency\n",
    "\n",
    "What to check:\n",
    "\n",
    " Verify that each frame (sample) in the dataset has all essential metadata fields. This typically includes timestamps, sensor identifiers, sensor calibration parameters (intrinsics and extrinsics), and ego-vehicle pose (position and orientation). Such metadata makes the dataset “complete” and is needed for downstream tasks. \n",
    " \n",
    "For example, nuScenes stores localization (ego pose), timestamps, and calibration data for each frame as part of its metadata, and FiftyOne likewise allows attaching any metadata you need (time of day, device ID, location, weather, etc.) to each sample. \n",
    "\n",
    "Without these, it’s difficult to fuse sensor data or interpret coordinates properly. Why it’s important: \n",
    "\n",
    " Incomplete metadata can lead to misinterpretation of the data (e.g., misaligning sensors or time). Ensuring consistency (e.g., units and coordinate frames) is equally important. Every sample should at least have a timestamp and ego-vehicle pose so that frames can be ordered and spatially related. Calibration info (camera intrinsics, sensor extrinsics relative to the car) must be present to project between sensor frames correctly. If any of these are missing or incorrect, the subsequent sanity checks and any sensor fusion algorithm might fail. \n",
    "\n",
    "How to check: \n",
    "\n",
    " Use FiftyOne to iterate through the dataset and assert the presence of required fields. For instance, suppose the dataset’s Sample schema includes fields like timestamp, ego_pose (with position/orientation), and per-sensor calibration entries (intrinsics/extrinsics). We can programmatically verify these. If something is missing, we should flag it before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the names of your corresponding fields as the values in the dictionaries below.\n",
    "\n",
    "metadata_dict = {\n",
    "    \"timestamp\" : \"timestamp\",                # Mandatory\n",
    "    \"intrinsics\" : \"camera_intrinsics\",       # Mandatory\n",
    "    \"T_rig_world\" : \"vehicle_to_world\",            # Mandatory\n",
    "    \"T_sensor_rig\" : \"camera_to_vehicle\",          # Mandatory\n",
    "}\n",
    "\n",
    "optional_metadata_dict = {\n",
    "    \"location\" : \"location\",                          # Optional\n",
    "    \"velocity\" : \"velocity\",                          # Optional\n",
    "    \"angular_velocity\" : \"angular_velocity\",          # Optional\n",
    "    \"s_timestamp\" : \"s_timestamp\",                    # Optional\n",
    "    \"e_timestamp\" : \"e_timestamp\",                    # Optional\n",
    "    \"rig_timestamp\" : \"rig_timestamp\",                # Optional\n",
    "    \"ground_truth_cuboids\" : \"ground_truth_cuboids\",  # Optional\n",
    "    \"ego_mask\" : \"ego_mask\",                          # Optional\n",
    "    \"ground_truth_masks\" : \"ground_truth_masks\",      # Optional\n",
    "    \"ground_truth_lidar_segmentation\" : \"ground_truth_lidar_segmentation\",  # Optional\n",
    "    \"ego_rotation\" : \"ego_rotation\",                  # Optional\n",
    "    \"cs_rotation\" : \"cs_rotation\",                    # Optional\n",
    "    \"ego_translation\" : \"ego_translation\",            # Optional\n",
    "    \"cs_translation\" : \"cs_translation\",              # Optional\n",
    "}\n",
    "\n",
    "expected_sensors = [\"pinhole_front\", \"pinhole_front_right\", \"pinhole_front_left\", \"pinhole_side_left\", \"pinhole_side_right\", \"3D\"] # Mandatory\n",
    "img_sensors = [\"pinhole_front\", \"pinhole_front_right\", \"pinhole_front_left\", \"pinhole_side_left\", \"pinhole_side_right\",]  # Mandatory\n",
    "pcd_sensors = [\"lidar\",]  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Check for Missing Metadata Fields\n",
    "\n",
    "In this step, we programmatically verify the presence of required metadata fields in the dataset schema. Missing metadata fields can lead to issues in downstream tasks, such as sensor fusion or coordinate interpretation.\n",
    "\n",
    "The `check_metadata_fields` function iterates through the dataset's schema and checks for the presence of mandatory fields defined in `metadata_dict`. If any required fields are missing, they are flagged for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_metadata_fields(dataset, metadata_dict):\n",
    "    # Get the field schema of the each slice\n",
    "\n",
    "    for sensor in expected_sensors:\n",
    "        view = dataset.select_group_slices(sensor)\n",
    "        field_schema = view.get_field_schema()\n",
    "        \n",
    "        # Check if each required metadata field exists in the schema\n",
    "        missing_fields = []\n",
    "        for key, field in metadata_dict.items():\n",
    "            if field not in field_schema:\n",
    "                missing_fields.append(key)\n",
    "        \n",
    "        # Return the result\n",
    "        if missing_fields:\n",
    "            print(f\"Missing metadata fields: {missing_fields} in {sensor} slice.\")\n",
    "        else:\n",
    "            print(\"All required metadata fields are present in the dataset schema.\")\n",
    "\n",
    "check_metadata_fields(dataset, metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Identify Present Optional Metadata Fields\n",
    "\n",
    "Optional metadata fields can provide additional context or insights but are not strictly necessary for all tasks. The `get_present_optional_fields` function checks which optional fields, as defined in `optional_metadata_dict`, are present in the dataset schema. This helps in understanding the dataset's completeness and potential for extended analysis.\n",
    "\n",
    "The variable `present_optional_fields` stores the list of optional fields that are available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_present_optional_fields(dataset, optional_metadata_dict):\n",
    "    # Get the field schema of the dataset\n",
    "\n",
    "    present_fields_dict = {}\n",
    "    for sensor in expected_sensors:\n",
    "        view = dataset.select_group_slices(sensor)\n",
    "        field_schema = view.get_field_schema()\n",
    "    \n",
    "        # Check if each optional metadata field exists in the schema\n",
    "        present_fields = []\n",
    "        for key, field in optional_metadata_dict.items():\n",
    "            if field in field_schema:\n",
    "                present_fields.append(key)\n",
    "        \n",
    "        # Return the result\n",
    "        if present_fields:\n",
    "            print(f\"Optional metadata fields present: {present_fields}\")\n",
    "        else:\n",
    "            print(\"No optional metadata fields are present in the dataset schema.\")\n",
    "        present_fields_dict[sensor] = present_fields\n",
    "    return present_fields_dict\n",
    "\n",
    "present_optional_fields = get_present_optional_fields(dataset, optional_metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Verify Sensor Data Presence and Synchronization\n",
    "\n",
    "What to check: \n",
    "\n",
    " Ensure that for each timestamp (or frame index), data from all expected sensors is present and correctly synchronized. For example, if the dataset should have 6 camera images and 1 LiDAR scan per frame, verify none of these modalities are missing. Each sample should contain data from every sensor in the rig (unless the sensor intentionally did not record for that frame). Additionally, confirm that the timestamps of different sensor data in the same frame are either identical or within an acceptable sync tolerance (depending on dataset specification). \n",
    " \n",
    "Why it’s important: \n",
    "\n",
    " Missing sensor data would create a “blind spot” in that frame’s perception. If one camera frame is dropped or out-of-sync, downstream algorithms might make incorrect assumptions. Many datasets ensure sensors are hardware-synchronized (e.g. Wayve’s rig of 5 cameras is time-synchronized at 10 Hz), so a missing or unsynced frame suggests an issue in conversion. Synchronized timestamps ensure that an image and LiDAR from the same sample depict the same moment in time. \n",
    " \n",
    "How to check: \n",
    "\n",
    " Iterate through the dataset and check each sample for the presence of each sensor’s data. If sensor data is stored in subfields (e.g., sample.camera_front, sample.camera_back, sample.lidar), verify those fields are populated. Also, check timestamp consistency across sensors in a frame (if sensors have separate timestamps). For synchronization, one approach is to ensure the difference between any sensor timestamp and the frame’s master timestamp is below a threshold.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sync_tolerance = 100000  # 100 ms tolerance for timestamp sync in microseconds\n",
    "\n",
    "\n",
    "missing_sensor_groups = []\n",
    "for group in dataset.iter_groups():\n",
    "\n",
    "    timestamp_list = []\n",
    "\n",
    "    # Check presence of each sensor and grab timestamp\n",
    "    for sensor in expected_sensors:\n",
    "        sample = group[sensor]\n",
    "        if sample is None:\n",
    "            print(f\"Missing data: {sensor} is not present for sample {group.id}\")\n",
    "            missing_sensor_groups.append((group.id, sensor))\n",
    "        else:\n",
    "            timestamp_list.append(sample[metadata_dict[\"timestamp\"]])\n",
    "\n",
    "    # Check synchronization of timestamps\n",
    "    if len(timestamp_list) > 1:\n",
    "        min_timestamp = min(timestamp_list)\n",
    "        max_timestamp = max(timestamp_list)\n",
    "\n",
    "        if max_timestamp - min_timestamp > max_sync_tolerance:\n",
    "            print(f\"Timestamp sync issue for group {sample.group.id}: \"\n",
    "                  f\"min {min_timestamp}, max {max_timestamp}, \"\n",
    "                  f\"difference {max_timestamp - min_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that each expected sensor is present in every sample. If a sensor data field is missing, it prints a message (indicating a potential conversion issue causing a blind spot). It also checks sensor timestamp vs. frame timestamp; if the difference exceeds max_sync_tolerance, it flags a synchronization issue. In a correctly converted dataset, we expect each frame to have all sensors present and timestamps closely aligned (often exactly the same timestamp for all modalities if the data were captured simultaneously)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: LiDAR-to-Image Projection Alignment (Extrinsic Calibration Check)\n",
    "\n",
    "What to check: \n",
    "\n",
    " Project the 3D LiDAR points into the 2D camera image and verify they align with the visual scene (i.e., points fall on the corresponding objects in the image). This checks that the LiDAR–camera extrinsic calibration (and the camera intrinsics) are correctly applied in the conversion. In practice, you overlay the point cloud on the image and see if, for example, points hit the surfaces of vehicles, pedestrians, and road in the image, rather than being offset. \n",
    " \n",
    "Why it’s important: \n",
    "\n",
    " Misalignments between LiDAR and camera indicate calibration errors or coordinate transform issues. If the transformation from LiDAR coordinates to camera coordinates is wrong, the projected points will not correspond to the correct image features. For instance, a misplaced rotation or translation could cause LiDAR points to appear shifted (e.g., hovering beside the actual object). Visualizing LiDAR over images is a common sanity check – misprojections can reveal calibration or time-sync problems. [In research literature](https://www.researchgate.net/figure/Examples-of-projecting-lidar-points-to-images-before-top-row-and-after-bottom-row_fig1_339813627), it’s noted that misalignment of lidar points and image features will be clearly visible if calibration is off or if motion distortion isn’t handled \n",
    " \n",
    "How to check: \n",
    "\n",
    " Use the known calibration parameters to transform LiDAR points to the camera frame and then apply the camera intrinsics to project to pixel coordinates. Then, visualize or statistically verify alignment. Using FiftyOne, one can write a custom loop or use utility functions if available. Below is an example using open3d and opencv to project points onto an image. We assume we have the camera intrinsic matrix K and a 4x4 homogeneous transform T_lidar_to_cam for the extrinsics (from LiDAR frame to camera frame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pyquaternion import Quaternion\n",
    "\n",
    "\n",
    "\n",
    "def transform_points(points, transform):\n",
    "    points_hom = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    \n",
    "    if not np.all(np.isfinite(points_hom)):\n",
    "        raise ValueError(\"Non-finite values in homogeneous points.\")\n",
    "\n",
    "    if not np.all(np.isfinite(transform)):\n",
    "        raise ValueError(\"Non-finite values in transform matrix.\")\n",
    "\n",
    "    # Check max magnitude (overflow-safe)\n",
    "    if np.abs(points_hom).max() > 1e6:\n",
    "        print(\"⚠️ Warning: Unusually large point values detected in transform_points input.\")\n",
    "\n",
    "    result = transform @ points_hom.T  # shape (4, N)\n",
    "\n",
    "    if not np.all(np.isfinite(result)):\n",
    "        print(\"⚠️ Warning: Non-finite values found after matrix multiplication.\")\n",
    "    \n",
    "    return result.T[:, :3]\n",
    "\n",
    "def translate(points, x: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a translation to the point cloud.\n",
    "        :param x: <np.float: 3, 1>. Translation in x, y, z.\n",
    "        \"\"\"\n",
    "        for i in range(3):\n",
    "            points[i, :] = points[i, :] + x[i]\n",
    "\n",
    "        return points\n",
    "\n",
    "def rotate(points, rot_matrix: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a rotation.\n",
    "        :param rot_matrix: <np.float: 3, 3>. Rotation matrix.\n",
    "        \"\"\"\n",
    "        points[:3, :] = np.dot(rot_matrix, points[:3, :])\n",
    "\n",
    "        return points\n",
    "\n",
    "def transform(points, transf_matrix: np.ndarray) -> None:\n",
    "        \"\"\"\n",
    "        Applies a homogeneous transform.\n",
    "        :param transf_matrix: <np.float: 4, 4>. Homogenous transformation matrix.\n",
    "        \"\"\"\n",
    "        points[:3, :] = transf_matrix.dot(np.vstack((points[:3, :], np.ones(points.shape[1]))))[:3, :]\n",
    "\n",
    "def view_points(points: np.ndarray, view: np.ndarray, normalize: bool) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This is a helper class that maps 3d points to a 2d plane. It can be used to implement both perspective and\n",
    "    orthographic projections. It first applies the dot product between the points and the view. By convention,\n",
    "    the view should be such that the data is projected onto the first 2 axis. It then optionally applies a\n",
    "    normalization along the third dimension.\n",
    "\n",
    "    For a perspective projection the view should be a 3x3 camera matrix, and normalize=True\n",
    "    For an orthographic projection with translation the view is a 3x4 matrix and normalize=False\n",
    "    For an orthographic projection without translation the view is a 3x3 matrix (optionally 3x4 with last columns\n",
    "     all zeros) and normalize=False\n",
    "\n",
    "    :param points: <np.float32: 3, n> Matrix of points, where each point (x, y, z) is along each column.\n",
    "    :param view: <np.float32: n, n>. Defines an arbitrary projection (n <= 4).\n",
    "        The projection should be such that the corners are projected onto the first 2 axis.\n",
    "    :param normalize: Whether to normalize the remaining coordinate (along the third axis).\n",
    "    :return: <np.float32: 3, n>. Mapped point. If normalize=False, the third coordinate is the height.\n",
    "    \"\"\"\n",
    "\n",
    "    assert view.shape[0] <= 4\n",
    "    assert view.shape[1] <= 4\n",
    "    assert points.shape[0] == 3\n",
    "\n",
    "    viewpad = np.eye(4)\n",
    "    viewpad[:view.shape[0], :view.shape[1]] = view\n",
    "\n",
    "    nbr_points = points.shape[1]\n",
    "\n",
    "    # Do operation in homogenous coordinates.\n",
    "    points = np.concatenate((points, np.ones((1, nbr_points))))\n",
    "    points = np.dot(viewpad, points)\n",
    "    points = points[:3, :]\n",
    "\n",
    "    if normalize:\n",
    "        points = points / points[2:3, :].repeat(3, 0).reshape(3, nbr_points)\n",
    "\n",
    "    return points\n",
    "\n",
    "\n",
    "def load_point_cloud_pcd(path):\n",
    "    pcd = o3d.io.read_point_cloud(path)\n",
    "    return np.asarray(pcd.points)\n",
    "\n",
    "def project_lidar_to_image_fiftyone(\n",
    "        img_sample,\n",
    "        pcd_sample,\n",
    "        pcd_path,\n",
    "        min_depth=1.0,\n",
    "        metadata_dict=metadata_dict,\n",
    "        optional_metadata_dict=optional_metadata_dict,\n",
    "        r_and_t_transform = False,\n",
    "        verbose=False,\n",
    "        pcd_coords=\"ego\"):\n",
    "    # Load point cloud\n",
    "    pts = load_point_cloud_pcd(pcd_path)\n",
    "    print(pcd_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Raw point cloud shape:\", pts.shape)\n",
    "        print(\"Any NaNs or infs?\", not np.all(np.isfinite(pts)))\n",
    "\n",
    "    # TO DO\n",
    "    if not r_and_t_transform:\n",
    "        # Load transforms\n",
    "        T_lidar_to_rig = np.array(pcd_sample[metadata_dict[\"T_sensor_rig\"]])  # 4x4\n",
    "        T_rig_to_world_lidar = np.array(pcd_sample[metadata_dict[\"T_rig_world\"]])  # 4x4\n",
    "\n",
    "        T_cam_to_rig = np.array(img_sample[metadata_dict[\"T_sensor_rig\"]]) # 4x4\n",
    "        T_rig_to_world_cam = np.array(img_sample[metadata_dict[\"T_rig_world\"]])  # 4x4\n",
    "\n",
    "\n",
    "        K = np.array(img_sample[\"intrinsics\"])  # 3x3\n",
    "\n",
    "        # Full chain: lidar -> rig -> world -> rig(cam) -> cam\n",
    "        T_world_to_rig_cam = np.linalg.inv(T_rig_to_world_cam)\n",
    "        T_rig_to_cam = np.linalg.inv(T_cam_to_rig)\n",
    "\n",
    "        T_lidar_to_cam = T_rig_to_cam @ T_world_to_rig_cam @ T_rig_to_world_lidar @ T_lidar_to_rig\n",
    "        pts_cam = transform_points(pts, T_lidar_to_cam)\n",
    "\n",
    "    else:\n",
    "\n",
    "        pts = pts.T # shape (3, N)\n",
    "        \n",
    "        # Points live in the point sensor frame. So they need to be transformed via global to the image plane.\n",
    "        # First step: transform the pointcloud to the ego vehicle frame for the timestamp of the sweep.\n",
    "\n",
    "        if pcd_coords == \"sensor\":\n",
    "            \n",
    "            pcd_sensor_rotation = pcd_sample[optional_metadata_dict[\"cs_rotation\"]]  \n",
    "            pcd_sensor_translation = pcd_sample[optional_metadata_dict[\"cs_translation\"]]\n",
    "            pts = rotate(pts, Quaternion(pcd_sensor_rotation).rotation_matrix)\n",
    "            pts = translate(pts, np.array(pcd_sensor_translation))\n",
    "\n",
    "        elif pcd_coords == \"ego\" or pcd_coords == \"sensor\":\n",
    "            # Second step: transform from ego to the global frame.\n",
    "            pcd_rig_rotation = pcd_sample[optional_metadata_dict[\"ego_rotation\"]]\n",
    "            pcd_rig_translation = pcd_sample[optional_metadata_dict[\"ego_translation\"]]\n",
    "            pts = rotate(pts, Quaternion(pcd_rig_rotation).rotation_matrix)\n",
    "            pts = translate(pts, np.array(pcd_rig_translation))\n",
    "\n",
    "        # Third step: transform from global into the ego vehicle frame for the timestamp of the image.\n",
    "        img_rig_rotation = img_sample[optional_metadata_dict[\"ego_rotation\"]]\n",
    "        img_rig_translation = img_sample[optional_metadata_dict[\"ego_translation\"]]\n",
    "        pts = translate(pts, -np.array(img_rig_translation))\n",
    "        pts = rotate(pts, Quaternion(img_rig_rotation).rotation_matrix.T)\n",
    "\n",
    "        # Fourth step: transform from ego into the camera.\n",
    "        img_sensor_rotation = img_sample[optional_metadata_dict[\"cs_rotation\"]]  \n",
    "        img_sensor_translation = img_sample[optional_metadata_dict[\"cs_translation\"]]\n",
    "        pts = translate(pts, -np.array(img_sensor_translation))\n",
    "        pts = rotate(pts, Quaternion(img_sensor_rotation).rotation_matrix.T)\n",
    "        print(pts.shape)\n",
    "\n",
    "        # Fifth step: actually take a \"picture\" of the point cloud.\n",
    "        # Grab the depths (camera frame z axis points away from the camera).\n",
    "        depths = pts[2, :]\n",
    "\n",
    "        coloring = depths\n",
    "\n",
    "        pts[1,:] = -pts[1, :]  # Flip y axis to match image coordinates (y down)\n",
    "\n",
    "        points = view_points(pts[:3, :], np.array(img_sample[metadata_dict['intrinsics']]), normalize=True)\n",
    "\n",
    "\n",
    "        im = Image.open(img_sample.filepath)\n",
    "        mask = np.ones(depths.shape[0], dtype=bool)\n",
    "        mask = np.logical_and(mask, depths > min_depth)\n",
    "        mask = np.logical_and(mask, points[0, :] > 1)\n",
    "        mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "        mask = np.logical_and(mask, points[1, :] > 1)\n",
    "        mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "        points = points[:, mask]\n",
    "        coloring = coloring[mask]\n",
    "\n",
    "        return points, coloring, im\n",
    "\n",
    "\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Camera-frame Z stats:\")\n",
    "        print(\"  Min Z:\", np.min(pts_cam[:, 2]))\n",
    "        print(\"  Max Z:\", np.max(pts_cam[:, 2]))\n",
    "        print(\"  Any Z ≤ 0?\", np.any(pts_cam[:, 2] <= 0))\n",
    "        print(\"  Any NaN or inf?\", not np.all(np.isfinite(pts_cam)))\n",
    "\n",
    "    # Filter points behind camera\n",
    "    mask = pts_cam[:, 2] > min_depth\n",
    "    pts_cam = pts_cam[mask]\n",
    "\n",
    "    # Project to image\n",
    "    projected = K @ pts_cam.T\n",
    "    projected = projected.T\n",
    "    # Z = depth (3rd column)\n",
    "    z = projected[:, 2]\n",
    "    valid = np.isfinite(z) & (z > min_depth)\n",
    "\n",
    "    projected = projected[valid]\n",
    "    z = z[valid][:, None]\n",
    "\n",
    "    pts_2d = projected[:, :2] / z\n",
    "\n",
    "    return pts_2d, pts_cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Overview\n",
    "\n",
    "This block of code updates and processes the intrinsic and transformation data for each sensor in the waymo dataset. Here's a breakdown:\n",
    "\n",
    "#### 1. **`fix_intrinsics` function**\n",
    "The `fix_intrinsics` function adjusts the camera intrinsic matrix based on the image's width and height. It:\n",
    "- Takes in a `sample` containing the camera intrinsic parameters.\n",
    "- **Scales** the intrinsic parameters (`fx`, `fy`, `cx`, `cy`) according to the actual image size.\n",
    "- Returns the updated intrinsic matrix.\n",
    "\n",
    "##### Steps:\n",
    "- **Extract Intrinsics**: It pulls the intrinsic parameters (`fx`, `fy`, `cx`, `cy`) and the original image size (`x`, `y`).\n",
    "- **Scale the Intrinsics**: It calculates scaling factors using the actual image dimensions (`sample.metadata.width`, `sample.metadata.height`).\n",
    "- **Apply Scaling**: The intrinsic parameters are multiplied by these scaling factors.\n",
    "- **Return Matrix**: It returns the new 3x3 intrinsic matrix in numpy array form.\n",
    "\n",
    "#### 2. **Looping Through Dataset and Sensor Groups**\n",
    "This block processes each sensor in the dataset:\n",
    "- **Rotation and Translation**:\n",
    "  - For each sensor (excluding the \"3D\" sensor), the rotation matrix `T_sensor_rig` is converted into a quaternion (`cs_rotation`), and the translation values are stored in `cs_translation`.\n",
    "- **Fix Intrinsics**: It applies the `fix_intrinsics` function and saves the result to the sensor’s `\"intrinsics\"` attribute.\n",
    "  \n",
    "- **Ego Frame Rotation and Translation**:\n",
    "  - The `T_rig_world` matrix is used to calculate the sensor's ego rotation and translation, which are saved as `ego_rotation` and `ego_translation`.\n",
    "\n",
    "- The updated sensor data is saved back into the dataset.\n",
    "\n",
    "#### 3. **Metadata Update**\n",
    "Finally, it updates the metadata to refer to the `\"intrinsics\"` key for the fixed intrinsic matrix.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Operations:\n",
    "- **Fix Intrinsics**: Scales the intrinsic parameters based on image resolution.\n",
    "- **Rotation and Translation**: Converts rotation matrices to quaternions and stores rotation and translation data.\n",
    "- **Saving Data**: Updates each sensor with the corrected intrinsic and transformation data.\n",
    "\n",
    "### In Summary:\n",
    "This code iterates over a dataset, fixes the camera intrinsic matrix for each sensor based on the image size, converts rotation matrices to quaternions, and saves the updated information to the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.transform import Rotation as R\n",
    "\n",
    "def fix_intrinsics(sample):\n",
    "    fy, fx, cy, cx, y, x = sample[\"camera_intrinsics\"]\n",
    "    scale_x = sample.metadata.width / x\n",
    "    scale_y = sample.metadata.height / y\n",
    "\n",
    "    fx *= scale_x\n",
    "    fy *= scale_y\n",
    "    cx *= scale_x\n",
    "    cy *= scale_y\n",
    "\n",
    "    return np.array([\n",
    "        [fx,  0, cx],\n",
    "        [0,  fy, cy],\n",
    "        [0,   0,  1]\n",
    "    ])\n",
    "\n",
    "for group in dataset.iter_groups():\n",
    "    for sensor_name, sensor in group.items():\n",
    "        if sensor_name != \"3D\": \n",
    "            r = R.from_matrix(sensor[metadata_dict[\"T_sensor_rig\"]][:3, :3])\n",
    "            q = r.as_quat()  \n",
    "            sensor[\"cs_rotation\"] = q\n",
    "            sensor[\"cs_translation\"] = sensor[metadata_dict[\"T_sensor_rig\"]][:3, 3]\n",
    "\n",
    "            sensor[\"intrinsics\"] = fix_intrinsics(sensor)\n",
    "            \n",
    "        r = R.from_matrix(sensor[metadata_dict[\"T_rig_world\"]][:3, :3])\n",
    "        q = r.as_quat()  \n",
    "        sensor[\"ego_rotation\"] = q\n",
    "        sensor[\"ego_translation\"] = sensor[metadata_dict[\"T_rig_world\"]][:3, 3]\n",
    "        sensor.save()\n",
    "\n",
    "metadata_dict[\"intrinsics\"] = \"intrinsics\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below lidar projection is currently bugged due to Waymo's diffrent coordinate system! See below to see debugging code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.core.threed as fotd\n",
    "\n",
    "for group in dataset.iter_groups():\n",
    "    for img_sensor in img_sensors:\n",
    "        img_sample = group[img_sensor]\n",
    "        pcd_sample = group[\"3D\"]\n",
    "\n",
    "        if img_sample is None or pcd_sample is None:\n",
    "            print(f\"Missing data: {img_sensor} or 3D for sample {group.id}\")\n",
    "            continue\n",
    "\n",
    "        fo3d_path = pcd_sample.filepath\n",
    "        pcd_paths = fo.Scene().from_fo3d(fo3d_path).get_asset_paths()\n",
    "        pcd_path = \"fo_waymo_dataset/data/\" + pcd_paths[0] if pcd_paths else None\n",
    "\n",
    "        if pcd_path is not None:\n",
    "            pts_2d, coloring,im = project_lidar_to_image_fiftyone(\n",
    "                img_sample,\n",
    "                pcd_sample,\n",
    "                pcd_path,\n",
    "                metadata_dict=metadata_dict,\n",
    "                r_and_t_transform=True,\n",
    "                pcd_coords=\"ego\",\n",
    "            )\n",
    "\n",
    "            if pts_2d.size == 0:\n",
    "                print(f\"No valid points projected for {img_sensor} in sample {img_sample}\")\n",
    "                continue\n",
    "\n",
    "            img_sample[\"lidar_points\"] = fo.Keypoints(\n",
    "            keypoints = [\n",
    "                fo.Keypoint(\n",
    "                    label=\"LIDAR_TOP\",\n",
    "                    points = list(zip(pts_2d.T[:, 0]/img_sample.metadata.width, pts_2d.T[:, 1]/img_sample.metadata.height)),\n",
    "                    colors = list(coloring)\n",
    "                )\n",
    "                ]\n",
    "            )\n",
    "            img_sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Overview\n",
    "\n",
    "This code is responsible for projecting 3D LIDAR points onto a 2D camera image using camera intrinsic parameters and transformations. Here's an explanation of each section of the code:\n",
    "\n",
    "#### 1. **Helper Functions**\n",
    "\n",
    "- **`print_points(points)`**:\n",
    "  - This function prints the minimum, maximum, and range of each coordinate (x, y, z) in the point cloud. It's useful for inspecting the distribution of 3D points before projecting them onto the image plane.\n",
    "\n",
    "- **`load_pcd(pcd_path)`**:\n",
    "  - This function reads a point cloud file (in `.pcd` format) from the provided path and returns it as a numpy array of shape (N, 3), where N is the number of points and each point has 3 coordinates (x, y, z).\n",
    "\n",
    "- **`make_K_matrix(fx, fy, cx, cy)`**:\n",
    "  - This function creates a 3x3 camera intrinsic matrix `K` from the provided focal lengths (`fx`, `fy`) and principal points (`cx`, `cy`). This matrix is used to project 3D points onto the 2D image plane.\n",
    "\n",
    "- **`transform_points(points, T)`**:\n",
    "  - This function applies a 4x4 transformation matrix `T` to a set of 3D points (Nx3). It first converts the points to homogeneous coordinates (Nx4), applies the transformation, and then converts them back to 3D.\n",
    "\n",
    "- **`project_to_image(points, K)`**:\n",
    "  - This function projects 3D points onto a 2D image plane using the camera intrinsic matrix `K`. It filters out points behind the camera (with z < 0) and returns the 2D image coordinates (`pts_2d`) and the corresponding depths.\n",
    "\n",
    "#### 2. **Main Workflow**\n",
    "\n",
    "- **Input Data**:\n",
    "  - `image_path`: Path to the camera image.\n",
    "  - `pcd_path`: Path to the point cloud data.\n",
    "\n",
    "- **Camera Intrinsics**:\n",
    "  - The camera intrinsics (focal lengths and principal points) are used to build the camera matrix `K`. The code also scales the intrinsic values based on the resized image dimensions.\n",
    "\n",
    "- **Point Cloud Loading**:\n",
    "  - The LIDAR point cloud is loaded from the specified path using the `load_pcd` function. The points are assumed to be in the vehicle (ego) frame.\n",
    "\n",
    "- **Transformation Matrices**:\n",
    "  - The code loads several transformation matrices:\n",
    "    - `T_vehicle_to_world_lidar`: Transformation from the vehicle frame to the world frame for LIDAR.\n",
    "    - `T_vehicle_to_world_cam`: Transformation from the vehicle frame to the world frame for the camera.\n",
    "    - `T_cam_to_vehicle`: Transformation from the camera frame to the vehicle frame.\n",
    "  - It then constructs a full transformation matrix (`T_lidar_to_cam`) to transform LIDAR points into the camera frame.\n",
    "\n",
    "- **Coordinate Transformation**:\n",
    "  - The LIDAR points are transformed to the camera frame using the `transform_points` function.\n",
    "  - The transformation matrix `R_waymo_to_nuscenes` is applied to convert the coordinates from Waymo to NuScenes' coordinate system, where the x and y axes are swapped.\n",
    "\n",
    "- **Depth and Projection**:\n",
    "  - A depth threshold (`min_depth = 1.0`) is applied to filter out points that are too close to the camera.\n",
    "  - The 3D points are projected onto the 2D image plane using the intrinsic matrix `K`.\n",
    "\n",
    "- **Masking Valid Points**:\n",
    "  - Points are masked based on depth and valid image coordinates (ensuring points are within the bounds of the image).\n",
    "\n",
    "#### 3. **Visualization**\n",
    "\n",
    "- **2D Plot**:\n",
    "  - The projected 2D points (`pts_2d`) are visualized on the camera image using `matplotlib`. The depth of each point is represented by its color in a colormap (`jet`).\n",
    "\n",
    "#### 4. **Code Summary**:\n",
    "- **Main Purpose**: This code projects 3D LIDAR points onto a 2D camera image, applying necessary transformations, scaling the camera intrinsics, and visualizing the result.\n",
    "- **Key Operations**:\n",
    "  1. Loading the point cloud and camera image.\n",
    "  2. Applying transformations to map LIDAR points to the camera frame.\n",
    "  3. Projecting the transformed points onto the 2D image.\n",
    "  4. Masking invalid points based on depth and image boundaries.\n",
    "  5. Visualizing the result by overlaying the projected LIDAR points on the camera image.\n",
    "\n",
    "This approach is common in applications where LIDAR data is fused with camera data for tasks like object detection, tracking, and scene reconstruction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import open3d as o3d\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def print_points(points):\n",
    "    \"\"\"Prints the min, max, and range of each coordinate in the point cloud.\"\"\"\n",
    "    x_min, x_max = points[:, 0].min(), points[:, 0].max()\n",
    "    y_min, y_max = points[:, 1].min(), points[:, 1].max()\n",
    "    z_min, z_max = points[:, 2].min(), points[:, 2].max()\n",
    "\n",
    "    print(f\"x_min: {x_min}, x_max: {x_max}\")\n",
    "    print(f\"y_min: {y_min}, y_max: {y_max}\")\n",
    "    print(f\"z_min: {z_min}, z_max: {z_max}\")\n",
    "\n",
    "def load_pcd(pcd_path):\n",
    "    pcd = o3d.io.read_point_cloud(pcd_path)\n",
    "    return np.asarray(pcd.points)  # (N, 3)\n",
    "\n",
    "def make_K_matrix(fx, fy, cx, cy):\n",
    "    return np.array([\n",
    "        [fx,  0, cx],\n",
    "        [0,  fy, cy],\n",
    "        [0,   0,  1]\n",
    "    ])\n",
    "\n",
    "def transform_points(points, T):\n",
    "    \"\"\"Applies a 4x4 transformation matrix to Nx3 points\"\"\"\n",
    "    points_h = np.hstack((points, np.ones((points.shape[0], 1))))  # Nx4\n",
    "    points_trans = (T @ points_h.T).T  # Nx4\n",
    "    return points_trans[:, :3]\n",
    "\n",
    "def project_to_image(points, K):\n",
    "    \"\"\"Projects 3D camera points to 2D image plane using intrinsics\"\"\"\n",
    "    points = points[points[:, 2] > 0]  # Keep only points in front\n",
    "    print_points(points)\n",
    "    pts_2d = (K @ points.T).T\n",
    "    pts_2d = pts_2d[:, :2] / pts_2d[:, 2:3]\n",
    "    return pts_2d, points[:, 2]\n",
    "\n",
    "# -- YOUR INPUTS --\n",
    "image_path = img_sample.filepath  # Path to the camera image\n",
    "pcd_path = pcd_path\n",
    "\n",
    "\n",
    "\n",
    "# Scale K to match resized image, e.g., 886 px height\n",
    "image = Image.open(image_path)\n",
    "\n",
    "\n",
    "# Load point cloud in vehicle (ego) frame\n",
    "pts_vehicle = load_pcd(pcd_path)  # Nx3\n",
    "\n",
    "# Load transforms\n",
    "T_vehicle_to_world_lidar = pcd_sample[metadata_dict[\"T_rig_world\"]]  # From pcd_sample['T_rig_world']\n",
    "T_vehicle_to_world_cam = img_sample[metadata_dict[\"T_rig_world\"]]    # From img_sample['T_rig_world']\n",
    "T_cam_to_vehicle = img_sample[metadata_dict[\"T_sensor_rig\"]]          # From img_sample['T_sensor_rig']\n",
    "T_vehicle_to_cam = np.linalg.inv(T_cam_to_vehicle)\n",
    "\n",
    "# Build full transform: LIDAR → world → vehicle_cam → camera\n",
    "T_world_to_vehicle_cam = np.linalg.inv(T_vehicle_to_world_cam)\n",
    "T_lidar_to_cam = T_vehicle_to_cam @ T_world_to_vehicle_cam @ T_vehicle_to_world_lidar\n",
    "\n",
    "\n",
    "# Transform points to camera frame\n",
    "pts_cam = transform_points(pts_vehicle, T_lidar_to_cam)\n",
    "\n",
    "R_waymo_to_nuscenes = np.array([\n",
    "    [0, 1, 0],   # X ← Y\n",
    "    [1, 0, 0],   # Y ← X\n",
    "    [0, 0, 1]    # Z ← Z\n",
    "])\n",
    "\n",
    "depths = pts_cam[:, 2]\n",
    "pts_cam = pts_cam @ R_waymo_to_nuscenes.T  # shape (N, 3)\n",
    "print(\"Transformed points shape:\", pts_cam.shape)\n",
    "\n",
    "pts_cam = pts_cam[:, :3].T  # shape (3, N)\n",
    "\n",
    "min_depth = 1.0  # Minimum depth threshold for valid points\n",
    "\n",
    "\n",
    "coloring = depths\n",
    "\n",
    "points = view_points(pts_cam[:3, :], np.array(img_sample[metadata_dict['intrinsics']]), normalize=True)\n",
    "\n",
    "print(points.shape)\n",
    "print(max(points[0, :]), min(points[0, :]))\n",
    "print(max(points[1, :]), min(points[1, :]))\n",
    "\n",
    "im = Image.open(img_sample.filepath)\n",
    "mask = np.ones(depths.shape[0], dtype=bool)\n",
    "mask = np.logical_and(mask, depths > min_depth)\n",
    "mask = np.logical_and(mask, points[0, :] > 1)\n",
    "#mask = np.logical_and(mask, points[0, :] < im.size[0] - 1)\n",
    "#mask = np.logical_and(mask, points[1, :] > 1)\n",
    "#mask = np.logical_and(mask, points[1, :] < im.size[1] - 1)\n",
    "pts_2d = points[:, mask]\n",
    "coloring = coloring[mask]\n",
    "print(pts_2d.shape)\n",
    "print(max(pts_2d[0, :]), min(pts_2d[0, :]))\n",
    "print(max(pts_2d[1, :]), min(pts_2d[1, :]))\n",
    "pts_2d = pts_2d.T\n",
    "\n",
    "# -- PLOT --\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(im)\n",
    "plt.scatter(pts_2d[:, 0], pts_2d[:, 1], c=depths, s=1, cmap='jet')\n",
    "plt.title(\"Projected LIDAR onto Camera Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intrinsics Check"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "driving_workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
